ConV 1D is a one dimensional convolution, so well take a five number window and multiply out the values in that window by the filter values in much the same way as image convolutions are done
	Let's say you have a small time series window with these values: [1, 2, 3, 4, 5]. This means the value 1 is at t=0, 2 is at t=1, etc.
	If you have a 1D kernel of size 3, then the first convolution will be for the values at [1, 2, 3] which are values for t=0 to t=2.
	When you pass this to the first timestep of the LSTM after the convolution, it means that the value at t=0 of the LSTM depends on t=1 and t=2 which are values into the future.
	For time series data, you want computations to only rely on current and previous time steps.
	One way to do that is to pad the array depending on the kernel size and stride. For a kernel size of 3 and stride of 1, the window can be padded as such: [0, 0, 1, 2, 3, 4, 5]. 1 is still at t=0 and two zeroes are prepended to simulate values in the past.
	This way, the first stride will be at [0, 0, 1] and this does not contain any future values when it is passed on to subsequent layers.
	The Conv1D layer does this kind of padding by setting padding=causal


# Reset states generated by Keras
tf.keras.backend.clear_session()

# Build the model
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(window_size,1)),
    tf.keras.layers.Conv1D(filters=64, kernel_size=3,
                      strides=1, padding="causal",
                      activation="relu"),
    tf.keras.layers.LSTM(64, return_sequences=True),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x: x * 400)
])

model_DNN = tf.keras.models.Sequential([
    tf.keras.Input(shape=(window_size,)),
    tf.keras.layers.Dense(30, activation="relu"), 
    tf.keras.layers.Dense(10, activation="relu"),
    tf.keras.layers.Dense(1)
])

model__CNN_RNN_DNN = tf.keras.models.Sequential([
    tf.keras.Input(shape=(window_size,1)),
    tf.keras.layers.Conv1D(filters=64, kernel_size=3,
                      strides=1,
                      activation="relu",
                      padding='causal'),
    tf.keras.layers.LSTM(64, return_sequences=True),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(30, activation="relu"),
    tf.keras.layers.Dense(10, activation="relu"),
    tf.keras.layers.Dense(1),
    tf.keras.layers.Lambda(lambda x: x * 400)
])

# Print the model summary
model.summary()

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
