If you are training a neural network especially in image processing, for various reasons it will usually learn better if you scale all values to between 0 and 1. It's a process called normalization

Sequential: That defines a sequence of layers in the neural network.

Flatten: Remember earlier where our images were a 28x28 pixel matrix when you printed them out? Flatten just takes that square and turns it into a 1-dimensional array.

Dense: Adds a layer of neurons

Each layer of neurons needs an activation function to tell them what to do. There are a lot of options, but just use these for now:
ReLU effectively means: return x if x > 0 else 0
Softmax takes a list of values and scales these so the sum of all elements will be equal to 1. When applied to model outputs, you can think of the scaled values as the probability for that class.

create your own custom callback

# Normalize the pixel values of the train and test images
training_images  = training_images / 255.0
test_images = test_images / 255.0

# Build the classification model
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(28,28)),
    tf.keras.layers.Flatten(),  # if not, you get an error about the shape of the data. The first layer in your network should be the same shape as your data. Right now our data is 28x28 images, and 28 layers of 28 neurons would be infeasible, so it makes more sense to 'flatten' that 28,28 into a 784x1
    tf.keras.layers.Dense(128, activation=tf.nn.relu),  # by adding more Neurons `tf.keras.layers.Dense(512, activation=tf.nn.relu),` we have to do more calculations, slowing down the process, but in this case they have a good impact -- we do get more accurate. That doesn't mean it's always a case of 'more is better', you can hit the law of diminishing returns very quickly!
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)  # the number of neurons in the last layer should match the number of classes you are classifying for, it is 10 here
])

model.compile(optimizer = tf.optimizers.Adam(),
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') >= 0.6): # Experiment with changing this value
            print("\nReached 60% accuracy so cancelling training!")
            self.model.stop_training = True
callbacks = myCallback()

model.fit(
	training_images,
	training_labels,
	epochs=5,
	callbacks=[callbacks]
)

# Evaluate the model on unseen data
model.evaluate(test_images, test_labels)